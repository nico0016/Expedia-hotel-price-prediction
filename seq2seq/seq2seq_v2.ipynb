{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "\n",
    "from scipy.stats import skew\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "from scipy.stats import chi2_contingency\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "import os\n",
    "os.chdir('/Users/liyuan/desktop/SI699')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Data_preprocess():\n",
    "    \n",
    "    def __init__(self, train_file_path, test_file_path):\n",
    "        self.train_file_path = train_file_path\n",
    "        self.test_file_path = test_file_path\n",
    "        self.all_data = pd.DataFrame()\n",
    "        self.sampled_data = pd.DataFrame()\n",
    "    \n",
    "    def load_data(self):\n",
    "        train = pd.read_csv(self.train_file_path)\n",
    "        test = pd.read_csv(self.test_file_path)\n",
    "        print('training data has %d records'%len(train))\n",
    "        print('test data has %d records'%len(test))\n",
    "        \n",
    "        # drop columns in training data that are not available in test data set, including :'position', 'click_bool', 'gross_bookings_usd', 'booking_bool'\n",
    "        cols_train_only = [col for col in train.columns.unique().tolist() if col not in test.columns.unique().tolist()]\n",
    "        print('Columns only available in training data:',cols_train_only)\n",
    "        train = train.drop(columns = cols_train_only)\n",
    "\n",
    "        # combine train and test data\n",
    "        self.all_data = pd.concat([train, test], ignore_index=True)\n",
    "        print('Whole dataset has %d records' % len(self.all_data))\n",
    "        \n",
    "        # convert 'date_time' to datatime object\n",
    "        self.all_data['date_time'] = pd.to_datetime(self.all_data.date_time)\n",
    "        self.all_data.sort_values(by=['date_time'],inplace=True)\n",
    "        self.all_data = self.all_data.reset_index(drop=True)\n",
    "        return self.all_data    \n",
    "    \n",
    "    def clean_data(self,data, output_file_name):\n",
    "        # handle NA values\n",
    "        NA_columns = []\n",
    "        for col in data.columns.unique().tolist():\n",
    "            if data[col].isna().values.any() == True:\n",
    "                NA_columns.append(col)\n",
    "        for col in NA_columns:\n",
    "            # create binary columns\n",
    "            new_col = 'new_'+ col\n",
    "            data[new_col] = data[col].apply(lambda x: 1 if x >= 0 else 0)\n",
    "        # replace old column NA values to median value\n",
    "        data = data.fillna(data.median())\n",
    "        # output to csv file\n",
    "        data.to_csv(output_file_name +'.csv',index = False, encoding = 'utf-8')\n",
    "        return data\n",
    "    \n",
    "    def sample_data(self,sample_size):\n",
    "        interval_range = len(self.all_data)//sample_size\n",
    "        mid_idx_lst = []\n",
    "        for i in range(1,sample_size+1):\n",
    "            mid_idx = (interval_range*(i-1) + interval_range*i)//2\n",
    "            mid_idx_lst.append(mid_idx)\n",
    "        self.sampled_data = self.all_data.iloc[mid_idx_lst]\n",
    "        return self.sampled_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training data has 9917530 records\n",
      "test data has 6622629 records\n",
      "Columns only available in training data: ['position', 'click_bool', 'gross_bookings_usd', 'booking_bool']\n",
      "Whole dataset has 16540159 records\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/envs/leetcode/lib/python3.6/site-packages/ipykernel_launcher.py:39: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    }
   ],
   "source": [
    "data_p = Data_preprocess('./expedia_data/train.csv','./expedia_data/test.csv')\n",
    "all_data = data_p.load_data()\n",
    "sampled_data = data_p.sample_data(5000)\n",
    "# cleaned_data = data_p.clean_data(all_data,'res/cleaned_data')\n",
    "cleaned_sampled_data = data_p.clean_data(sampled_data,'res/cleaned_sampled_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Reformat_data():\n",
    "    def __init__(self):\n",
    "        self.data = pd.DataFrame()\n",
    "        self.variables = []\n",
    "        self.categorical_vars = []\n",
    "        self.categorical_binary_vars = []\n",
    "        self.continuous_vars = []\n",
    "        self.dest_data_list = {}\n",
    "        self.datetime_range = []\n",
    "        # list destination id with enough data\n",
    "        self.valid_dest_list = []\n",
    "        self.daily_price_data = []\n",
    "        # self.all_daily_price = pd.DataFrame()\n",
    "        self.all_daily_price = {}\n",
    "    \n",
    "    def load_data(self, data_file_path):\n",
    "        self.data = pd.read_csv(data_file_path, encoding = 'utf-8')\n",
    "        return self.data\n",
    "        \n",
    "    def divide_variables(self):\n",
    "        self.variables += [col for col in self.data.columns.unique().tolist() if col not in ['price_usd','date_time']]\n",
    "        self.categorical_vars += ['srch_id','site_id','visitor_location_country_id','visitor_hist_starrating','prop_country_id','prop_id','prop_starrating',\n",
    "        'srch_destination_id']\n",
    "        other_cols = [col for col in self.variables if col not in self.categorical_vars]\n",
    "        # get categorical binary variables\n",
    "        self.categorical_binary_vars += ['promotion_flag']\n",
    "        self.categorical_binary_vars += [col for col in self.data if col.startswith('new')]\n",
    "        self.categorical_binary_vars += [col for col in self.data if col.endswith('inv')]\n",
    "        self.categorical_binary_vars += [col for col in self.data if col.endswith('bool')]\n",
    "        # get continous variables\n",
    "        self.continuous_vars += [ col for col in variables if (col not in categorical_binary_vars) & (col not in categorical_vars )]\n",
    "        return self.categorical_vars,self.categorical_binary_vars,self.continuous_vars\n",
    "    \n",
    "    def get_data_by_dest(self):\n",
    "        '''separate entire dataset by destinations; append to a list'''\n",
    "        srch_destination_ids = self.data['srch_destination_id'].unique().tolist()\n",
    "        for srch_destination_id in srch_destination_ids:\n",
    "            destination_data = self.data[self.data['srch_destination_id'] == srch_destination_id]\n",
    "            self.dest_data_list[srch_destination_id] = destination_data\n",
    "        return self.dest_data_list\n",
    "    \n",
    "    def get_datetime_range(self):\n",
    "        # covert 'date_time' to datetime object\n",
    "        self.data['date_time'] = pd.to_datetime(self.data.date_time)\n",
    "        # resample by day\n",
    "        data = self.data.set_index('date_time')\n",
    "        price_data = data['price_usd'].resample('D').median()\n",
    "        self.datetime_range += price_data.index.tolist()\n",
    "        return self.datetime_range\n",
    "    \n",
    "    def get_daily_price(self, dest_id):\n",
    "        dest_data = self.dest_data_list[dest_id]\n",
    "        dest_data['date_time'] = pd.to_datetime(dest_data.date_time)\n",
    "        dest_data = dest_data.set_index('date_time')\n",
    "        dest_daily_price = dest_data['price_usd'].resample('D').median()\n",
    "        return dest_daily_price\n",
    "    \n",
    "    def get_valid_dests(self):\n",
    "        '''get a list of destination ids covering at least 50% of the datetime range'''\n",
    "        for dest_id in self.dest_data_list.keys():\n",
    "            # after remove na, if num of available records exceed 50% percentile of all date range, then keep it; otherwise, remove destination from dataset\n",
    "            if len(self.get_daily_price(dest_id).dropna()) >= len(self.datetime_range)*0.5:\n",
    "                self.valid_dest_list.append(dest_id)\n",
    "        return self.valid_dest_list\n",
    "    \n",
    "#     def concatenate_daily_price(self):\n",
    "#         '''concatenate daily price data of all destinations'''\n",
    "#         # TODO: need to merge multiple \n",
    "#         for dest_id in self.dest_data_list.keys():\n",
    "#             self.daily_price_data.append(self.get_daily_price(dest_id))\n",
    "#         self.all_daily_price = pd.concat(self.daily_price_data, ignore_index=True) \n",
    "#         return self.all_daily_price  \n",
    "    \n",
    "    def get_all_daily_price(self):\n",
    "        for dest_id in self.dest_data_list.keys():\n",
    "            self.all_daily_price[dest_id] = self.get_daily_price(dest_id)\n",
    "        return self.all_daily_price\n",
    "\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "there are 2247 unique destinations\n",
      "there are 242 days\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/envs/leetcode/lib/python3.6/site-packages/ipykernel_launcher.py:53: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    }
   ],
   "source": [
    "data_r = Reformat_data()\n",
    "sampled = data_r.load_data('res/sampled_data_5000.csv')\n",
    "categorical_vars,categorical_binary_vars,continuous_vars = data_r.divide_variables()\n",
    "dest_data_list = data_r.get_data_by_dest()\n",
    "print('there are %d unique destinations' % len(dest_data_list))\n",
    "datetime_range = data_r.get_datetime_range()\n",
    "print('there are %d days'%len(datetime_range))\n",
    "# valid_dest_list = data_r.get_valid_dests()\n",
    "# all_daily_price = data_r.concatenate_daily_price()\n",
    "all_daily_price = data_r.get_all_daily_price()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[17894, 19274, 4453, 11485, 3404, 11299, 9265, 23904, 24844, 20834]\n",
      "sorted list of destinations rankded in desceding of valid record number (top10):\n",
      "[8192, 4562, 10979, 9402, 23904, 13292, 8347, 6948, 13870, 13216]\n",
      "largest number of valid records among all destinations: 59\n"
     ]
    }
   ],
   "source": [
    "print(list(all_daily_price.keys())[:10])\n",
    "# sort price data by destination, based on their valid record number\n",
    "print('sorted list of destinations rankded in desceding of valid record number (top10):')\n",
    "print(sorted(all_daily_price.keys(), key = lambda x: len(all_daily_price[x].dropna()), reverse = True)[:10])\n",
    "print('largest number of valid records among all destinations: %d'%len(all_daily_price[8192].dropna()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "class seq2seq():\n",
    "    def __init__(self):\n",
    "        self.data = pd.DataFrame()\n",
    "        # subset of data divided by week\n",
    "        self.sequence_list = []\n",
    "        # number of weeks cover by data\n",
    "        self.weeks = 0\n",
    "        self.extra_day = 0\n",
    "    \n",
    "    def load_data(self, file_path):\n",
    "        self.data = pd.read_csv(file_path)\n",
    "        return self.data\n",
    "            \n",
    "    def get_sequence_data(self):\n",
    "        '''process data into seven days as a sequence'''\n",
    "        # TODO: predict next 7 days' price based on previous 7 days\n",
    "        self.weeks += len(self.data)//7\n",
    "        self.extra_day += len(self.data) % 7\n",
    "        i = 1\n",
    "        while 1 <= i <= self.weeks:\n",
    "            subset = self.data[(i-1)*7:i*7]\n",
    "            self.sequence_list.append(subset)\n",
    "            i+=1\n",
    "        return self.sequence_list\n",
    "\n",
    "    def implement_seq2seq():\n",
    "        pass\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 80.,  nan,  nan,  nan, 299., 215.,  nan])"
      ]
     },
     "execution_count": 233,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ss = seq2seq()\n",
    "data = ss.load_data('res/price_data_8192.csv')\n",
    "sequence_list = ss.get_sequence_data()\n",
    "\n",
    "sequence_list[0]['price'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import helpers\n",
    "\n",
    "tf.reset_default_graph()\n",
    "sess = tf.InteractiveSession()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.12.0'"
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "PAD = 0\n",
    "EOS = 1\n",
    "\n",
    "vocab_size = 10\n",
    "input_embedding_size = 20\n",
    "\n",
    "encoder_hidden_units = 20\n",
    "decoder_hidden_units = encoder_hidden_units\n",
    "\n",
    "encoder_inputs = tf.placeholder(shape=(None, None), dtype=tf.int32, name='encoder_inputs')\n",
    "decoder_targets = tf.placeholder(shape=(None, None), dtype=tf.int32, name='decoder_targets')\n",
    "\n",
    "decoder_inputs = tf.placeholder(shape=(None, None), dtype=tf.int32, name='decoder_inputs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
